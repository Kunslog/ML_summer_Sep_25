{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kunslog/ML_summer_Sep_25/blob/main/template_conformal_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #2 Conformal inference\n",
        "\n",
        "**Oxford University Economics Summer School 2025, Foundations of Machine Learning**\n",
        "\n",
        "*Johanna Barop, 2025*\n",
        "\n",
        "These are my solutions to the second problem set for the *Foundations of Machine Learning* course at the **Oxford University Economics Summer School 2025**.\n"
      ],
      "metadata": {
        "id": "DSqQzSVrQH0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this problem, you are asked to implement and evaluate some **conformal inference** procedures in Python. This problem builds on the first coding exercise, using Scikit-learn.\n",
        "\n",
        "You may want to consult [Angelopoulos, A. N. and Bates, S. (2021)](https://arxiv.org/pdf/2107.07511), \"A gentle introduction to conformal prediction and distribution-free uncertainty quantification\"."
      ],
      "metadata": {
        "id": "M25VblaKU4ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conformal inference\n",
        "Conformal inference is a method of uncertainty prediction. Normally we predict outcomes. Now, we will predict a _set_ that is guaranteed to contain the correct outcome in at least $(1-\\alpha)$% of cases.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TPeP8fvkhn3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How does conformal inference work intuitively?"
      ],
      "metadata": {
        "id": "YZ4XOM7eunHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **goal** of conformal inference is uncertainty quantification through generating _prediction sets_.\n",
        "\n",
        "![prediction-set-example.jpg](https://drive.google.com/uc?export=view&id=178JhIp_t4JkeToHP4x-q7frFGd4re_DN)\n",
        "\n",
        "Figure taken from: Angelopoulos, A. N. and Bates, S. (2021). A gentle introduction to conformal prediction and distribution-free uncertainty quantification. arXiv preprint arXiv:2107.07511.\n",
        "\n",
        "All other figures are taken from the same paper."
      ],
      "metadata": {
        "id": "TE5ezZyYlOw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conformal inference recipe**\n",
        "\n",
        "1. Train your model.\n",
        "2. For each point $(x_i, y_i)$ in\n",
        "the calibration set, compute the conformity score $s(x_i, y_i)$.\n",
        "3. For a desired confidence\n",
        "level $(1-\\alpha)$ (e.g., $\\alpha$ = 0.1 for 90% confidence), find the $\\frac{(1 − \\alpha)(n + 1)}{n}$-th quantile of\n",
        "the calibration scores, where $n$ is the size of the calibration set.\n",
        "4. For each test point $x$, construct the prediction set $C(x) = \\{y : s(x, y) ≥ \\text{quantile}\\}$.\n",
        "\n",
        "![illustration-conformal-inference.jpg](https://drive.google.com/uc?export=view&id=1tQJFM3JmzES1uYZrmlj-3m6btuXTX0_6)"
      ],
      "metadata": {
        "id": "71M96DmkpNpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How will we implement conformal inference?"
      ],
      "metadata": {
        "id": "2AO8RAxXvMnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conformal inference works for continuous and discrete data, and we will study **examples** of\n",
        "- continuous outcome regression\n",
        "- quantile regression\n",
        "- discrete classification.\n",
        "\n",
        "For each of these examples, we'll follow the following **steps**:\n",
        "1. Load the data sets and split them into training, calibration, and test set. Train the predictors.\n",
        "2. Compute _scores_ and _confidence sets_.\n",
        "3. Summarise our results. We'll compute coverage rates, interval widths (regression) or prediction set sizes (classification), and visualise the prediction sets for selected examples.\n",
        "4. Discuss strengths and limitations of the approach and compare results across different methods and data sets.\n",
        "\n",
        "We'll work on steps 1 and 2 today and then finish the problem set up tomorrow."
      ],
      "metadata": {
        "id": "IFd8QZOVTzvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "JpnjrFFfQVO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load libraries and data sets - it's good practice to do this at the top of your script."
      ],
      "metadata": {
        "id": "MKQHVe6slmzz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DnNnh5Lw7bx7"
      },
      "outputs": [],
      "source": [
        "# Import all necessary libraries here.\n",
        "\n",
        "# Import the wine and california_housing data sets from sklearn.datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPyGdErC7bx8"
      },
      "source": [
        "# Define functions to calculate scores and confidence sets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll define functions at the top because that's best practice. Fill in these functions as you work on the examples!"
      ],
      "metadata": {
        "id": "TdqHTSC8j6NK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cV5daZD7bx9"
      },
      "source": [
        "First, define the helper function `alphatilde(alpha, n)` for rounding the size cutoff appropriately. It computes\n",
        "$$\n",
        "\\tilde\\alpha = 1-\\frac{\\lceil(n+1)(1-\\alpha)\\rceil}{n}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "LqnFhuy57bx-"
      },
      "outputs": [],
      "source": [
        "# Define alphatilde().\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next define functions to compute the score thresholds and prediction sets for continuous regression, quantile regression, and discrete classification."
      ],
      "metadata": {
        "id": "cztRCL5Hbbc7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO0wRtJO7bx_"
      },
      "source": [
        "### Continuous outcomes, simple score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the conformity score as:\n",
        "$$\n",
        "s(x, y) = |y − \\hat f(x)|\n",
        "$$\n",
        "where $\\hat f(x)$ is the predicted value from the ridge regression model."
      ],
      "metadata": {
        "id": "oi51NTdT1jJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For evaluation, construct prediction intervals $[\\hat f(x) − q, \\hat f(x) + q]$ where $q$ is the appropriate quantile of calibration residuals."
      ],
      "metadata": {
        "id": "fq0Wo2IE120M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1-G7R_g7byA"
      },
      "source": [
        "### Quantile regression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the conformity score as:\n",
        "$$\n",
        "s(x, y) = \\max(t_{\\alpha/2}(x)− y, y − t_{1-\\alpha/2}(x))\n",
        "$$\n",
        "\n",
        "where $t_{\\alpha/2}(x)$ and $t_{1-\\alpha/2}(x)$ are the predicted quantiles.\n",
        "\n",
        "The conformity score measures how far the true value $y$ falls outside the predicted quantile interval, with negative scores indicating the point lies within the predicted interval."
      ],
      "metadata": {
        "id": "5NY-2lDv2o-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intuitively, $\\hat q$ grows or shrinks the distance between the quantiles to achieve coverage:\n",
        "\n",
        "![quantile-regression-illustration.jpg](https://drive.google.com/uc?export=view&id=1yl2-RCoPlPBfmaGYDc6WnRDlhtMZSmsv)"
      ],
      "metadata": {
        "id": "kOFNhYF_w3-h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw8hR9EI7bx-"
      },
      "source": [
        "### Discrete classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the conformity score as:\n",
        "\n",
        "$$\n",
        "s(x,y) = \\sum_{y \\prime} \\mathbb{1} ( j(y|x) \\ge  j(y^\\prime|x ) ) \\cdot \\hat f(y|x)\n",
        "$$\n",
        "\n",
        "where $\\mathbb{1}(\\cdot)$ is the indicator function. $\\hat f(y|x)$ represents the predicted probability of class $y$ given features $x$. $j(y, x)$ is the rank of $\\hat f(y|x)$  across all possible labels $y$.\n",
        "\n",
        "![prediction-sets-classification.jpg](https://drive.google.com/uc?export=view&id=1I0fMj-uZEkL5l7cT6U7WrgYn942o0_T0)"
      ],
      "metadata": {
        "id": "tZxraYSWprRN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpXEt5eA7byB"
      },
      "source": [
        "# Empirical examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZPR88LO7byC"
      },
      "source": [
        "## Continuous regression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the `california_housing` data and split it into features X and labels Y.\n",
        "\n",
        "\n",
        "\n",
        "For a documentation of the data, see: https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset"
      ],
      "metadata": {
        "id": "cHddrddgcQpY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u1Flf9zSiMS"
      },
      "source": [
        "Split the data into three subsets:\n",
        "- a **training set** (60%) for initial model fitting,\n",
        "- a **calibaration set** (20%) used for conformal inference calibration,\n",
        "- and a **validation set** (20%) for final evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "iejHj8HD7byD"
      },
      "outputs": [],
      "source": [
        "# Load the data and separate the data set into features X and outcomes y.\n",
        "from sklearn.datasets import load_wine, fetch_california_housing\n",
        "from sklearn.preprocessing import StandardScaler # For normalizing regressors\n",
        "\n",
        "data1 = fetch_california_housing()\n",
        "X = data1[\"data\"]\n",
        "X = StandardScaler().fit_transform(X)\n",
        "y = data1[\"target\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_cal, X_val, y_cal, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training, calibration, and validation.\n",
        "# Hint: Do this in two steps using train_test_split()."
      ],
      "metadata": {
        "id": "lUZQ3EabwpiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a _ridge regression_ with penalty parameter selected via cross-validation. Use cross-validation on the training set to find the optimal penalty parameter. (See the \"Supervised Learning\" PS for guidance.)"
      ],
      "metadata": {
        "id": "0eSlPRHHcAlo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPncvnr-7byD"
      },
      "outputs": [],
      "source": [
        "# Define the parameter range.\n",
        "\n",
        "# Run a grid search with cross-validation.\n",
        "\n",
        "# Select the best model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the predictions on calibration and validation data sets."
      ],
      "metadata": {
        "id": "UWb4CsKDcv3y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yMg7xX57byD"
      },
      "outputs": [],
      "source": [
        "# Compute the predictions on calibration and validation data sets.\n",
        "# Make sure to store them for future reference."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the threshold as the quantile of the calibration scores. Then obtain the prediction sets for the validation data."
      ],
      "metadata": {
        "id": "Y62syklLdF6e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OJNwPT07byD"
      },
      "outputs": [],
      "source": [
        "# Compute the thresholds.\n",
        "# Then use the thresholds to obtain the prediction sets for the validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aafdd172"
      },
      "source": [
        "# Print the shape and the first few entries of the prediction sets to check your work."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the average interval width and compute empirical coverage on the validation data.\n",
        "\n",
        "![coverage-illustration.jpg](https://drive.google.com/uc?export=view&id=1ciYTYrnb7F3xxqyVGTKsmL5UCgqUlMPl)"
      ],
      "metadata": {
        "id": "50u-CXtp0-CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average interval width\n",
        "\n",
        "# Coverage\n"
      ],
      "metadata": {
        "id": "zXnkjB0j4xTK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What do set sizes tell us?**\n",
        "\n",
        "- **Set size:**\n",
        "- **Spread of set size** (i.e., some sets are small, others are large):"
      ],
      "metadata": {
        "id": "0tu8Tq7G1E_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot prediction intervals for a subset of validation points and compare them to the true values."
      ],
      "metadata": {
        "id": "8zE4T0uzlnF5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khvi4TGk2YiV"
      },
      "source": [
        "# Create your plot here to visualise your work. Hint: Use matplotlib.pyplot.scatter() and matplotlib.pyplot.plot()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the distribution of interval width over the validation set. What do you notice?"
      ],
      "metadata": {
        "id": "DcDdPsH5yIW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create your plot here. Hint: Use matplotlib.pyplot.hist()"
      ],
      "metadata": {
        "id": "Xsv-0ei1vA5Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9ntVH3z7byD"
      },
      "source": [
        "## Quantile regression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continue using the `california_housing` data."
      ],
      "metadata": {
        "id": "gf0Qq2XtdMhi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMnUAL5A7byE"
      },
      "source": [
        "Fit quantile regression models for quantiles $\\tau_{\\alpha/2}$ and $\\tau_{1-\\alpha/2}$ (e.g., 0.05 and 0.95 for $\\alpha$ = 0.1). For simplicity, use fixed L1 regularisation and no tuning.\n",
        "Note that you can use scikit-learn’s `QuantileRegressor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rA136BlK7byE"
      },
      "outputs": [],
      "source": [
        "# Fit quantile regression models for lower and upper quantiles."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the predictions on calibration and validation data sets."
      ],
      "metadata": {
        "id": "hx3IckchfN1Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jxuEqsC77byF"
      },
      "outputs": [],
      "source": [
        "# Compute the predictions on calibration and validation data sets.\n",
        "# Make sure to store them for future reference."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the threshold as the quantile of the calibration scores. Then obtain the prediction sets for the validation data."
      ],
      "metadata": {
        "id": "DaULMMJlfQKm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wbgMs4WlzdhM"
      },
      "outputs": [],
      "source": [
        "# Compute the thresholds.\n",
        "# Then use the thresholds to obtain the prediction sets for the validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dlj7q2BzdhN"
      },
      "source": [
        "# Print the shape and the first few entries of the prediction sets to check your work."
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare coverage and interval widths with the ridge regression approach from the\n",
        "previous problem. Analyze whether quantile regression provides more adaptive intervals and discuss the trade-offs between the two approaches."
      ],
      "metadata": {
        "id": "E8KEC69o3aEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average interval width\n",
        "\n",
        "# Coverage"
      ],
      "metadata": {
        "id": "W03KOMna4vQy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot prediction intervals for a subset of validation points and compare them to the true values."
      ],
      "metadata": {
        "id": "WYSFXM4jzvBs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcHYGg4Ap8mH"
      },
      "source": [
        "# Create your plot here."
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the distribution of interval width over the validation set. What do you notice? Compare your results to the simple regression."
      ],
      "metadata": {
        "id": "p5LBcCokz8la"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kADw8eN-21xf"
      },
      "source": [
        "# Create your plot here."
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCPWaHZi7byB"
      },
      "source": [
        "## Discrete classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the `wine` data from the skikit-learn package and split it into features $X$ and labels $Y$.\n",
        "\n",
        "For a documentation of the data, see: https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-recognition-dataset."
      ],
      "metadata": {
        "id": "UoxX0AeUUr0D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "fdpr_8yW0lz2"
      },
      "outputs": [],
      "source": [
        "# Load the data and separate the data set into features X and outcomes y.\n",
        "\n",
        "# Import all necessary libraries here.\n",
        "import numpy as np\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler # For normalizing regressors\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression, Ridge, QuantileRegressor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training, calibration, and validation.\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Continuous outcomes, simple regression\n",
        "\n",
        "data_wine = load_wine()\n",
        "X = data_wine[\"data\"]\n",
        "X = StandardScaler().fit_transform(X)\n",
        "y = data_wine[\"target\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_cal, X_val, y_cal, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "IJ103VwB0lz4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic regression"
      ],
      "metadata": {
        "id": "av1b34J0TGvG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4uf7AW-7byC"
      },
      "source": [
        "Fit a penalized logistic regression model using scikit-learn’s `LogisticRegression` with L2 regularization. Use cross-validation on the training set to find the optimal penalty parameter $\\lambda$  that minimizes classification error.\n",
        "\n",
        "This will give the scores for _conformal inference_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "rdzV2OBG0065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "aa31554e-c33b-489f-d63e-25124dea064a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Define the parameter range.\n",
        "model = LogisticRegression()\n",
        "model.fit(Xtrain, ytrain)\n",
        "y_predicted_test = model.predict(Xtest)\n",
        "accuracy_score(ytest, y_predicted_test)\n",
        "\n",
        "# Select the best model.\n",
        "mat = confusion_matrix(ytest, y_predicted_test)\n",
        "from seaborn import heatmap\n",
        "heatmap(mat, square=True, annot=True, cbar=False, cmap='Blues')\n",
        "plt.xlabel('Predicted value')\n",
        "plt.ylabel('True value');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the predictions on calibration and validation data sets."
      ],
      "metadata": {
        "id": "xEhVekBf0066"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RKpcgUdM0067"
      },
      "outputs": [],
      "source": [
        "# Compute the predictions on calibration and validation data sets.\n",
        "# Make sure to store them for future reference."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the threshold as the quantile of the calibration scores. Then obtain the prediction sets for the validation data."
      ],
      "metadata": {
        "id": "R2wB1SPUZ4U9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_AxaiPS81EMV"
      },
      "outputs": [],
      "source": [
        "# Compute the thresholds.\n",
        "# Then use the thresholds to obtain the prediction sets for the validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG3QKblw1EMW"
      },
      "source": [
        "# Print the shape and the first few entries of the prediction sets to check your work."
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the method on the validation data set by computing:\n",
        "- average set size (mean\n",
        "number of labels in the prediction sets)\n",
        "- coverage (fraction\n",
        "of test points where the true label is in the prediction set)\n",
        "- conditional coverage (coverage rates\n",
        "for each true class): Is the coverage at least 90% for each type of wine?"
      ],
      "metadata": {
        "id": "zjiNY7dFsK40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average set size\n",
        "\n",
        "# Coverage: is y_val[i] contained in confidencesets_classification[i]?\n",
        "\n",
        "# Conditional coverage\n"
      ],
      "metadata": {
        "id": "yOJK7PFwsRnq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### k-Nearest Neighbours"
      ],
      "metadata": {
        "id": "O6Em63TuS4_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat the conformal inference procedure from the previous problem, but replace the\n",
        "penalized logistic regression with **k-nearest neighbors classification**. Use cross-validation on the training set to select the optimal number of neighbors $k$. Use the same conformity score definition as in the previous problem. Compare the coverage and efficiency (average prediction set size) with the logistic regression results, and discuss any differences in performance between the two methods."
      ],
      "metadata": {
        "id": "sQiVbfmj0oDP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "0VinvJQf1bJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae30ad7d-a4d1-4dc4-f8b9-f06d40565acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_neighbors': np.int64(15)}\n"
          ]
        }
      ],
      "source": [
        "# Define the parameter range.\n",
        "# Run a grid search with cross-validation.\n",
        "# Select the best model.\n",
        "# Perform a grid search with cross-validation to select the optimal number of neighbours.\n",
        "# Import all necessary libraries here.\n",
        "import numpy as np\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler # For normalizing regressors\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression, Ridge, QuantileRegressor\n",
        "\n",
        "# Import the wine and california_housing data sets from sklearn.datasets.\n",
        "from sklearn.datasets import load_wine, fetch_california_housing\n",
        "\n",
        "data2 = load_wine()\n",
        "X = data2[\"data\"]\n",
        "X = StandardScaler().fit_transform(X)\n",
        "y = data2[\"target\"]\n",
        "\n",
        "# Split into train and test set.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "# Split the test set into calibration and validation.\n",
        "X_cal, X_val, y_cal, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "# kNN\n",
        "\n",
        "# Perform a grid search with cross-validation to select the optimal number of neighbours.\n",
        "k_range = np.arange(1, 50)\n",
        "kNN_cv = GridSearchCV(KNeighborsClassifier(), {'n_neighbors': k_range}, cv=10, scoring=\"accuracy\")\n",
        "with warnings.catch_warnings(): # Suppress output of warnings\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    kNN_cv.fit(X_train, y_train)\n",
        "print(kNN_cv.best_params_)\n",
        "bestkNN = kNN_cv.best_estimator_\n",
        "\n",
        "y_calproba = bestkNN.predict_proba(X_cal) # proba returns the probability estimates for each class rather than just the predicted class label\n",
        "y_valproba = bestkNN.predict_proba(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the predictions on calibration and validation data sets."
      ],
      "metadata": {
        "id": "LxEtYJq91bJF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wiOA3jw_1bJG"
      },
      "outputs": [],
      "source": [
        "# Compute the predictions on calibration and validation data sets.\n",
        "# Make sure to store them for future reference."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the threshold as the quantile of the calibration scores. Then obtain the prediction sets for the validation data."
      ],
      "metadata": {
        "id": "YM7Jx_4y1bJG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LhDMJDpE1bJG"
      },
      "outputs": [],
      "source": [
        "# Compute the thresholds.\n",
        "# Then use the thresholds to obtain the prediction sets for the validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZjRdU4V1bJH"
      },
      "source": [
        "# Print the shape and the first few entries of the prediction sets to check your work."
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the method on the validation data set by computing:\n",
        "- average set size (mean\n",
        "number of labels in the prediction sets)\n",
        "- coverage (fraction\n",
        "of test points where the true label is in the prediction set)\n",
        "- conditional coverage (coverage rates\n",
        "for each true class): Is the coverage at least 90% for each type of wine?"
      ],
      "metadata": {
        "id": "1iRymW7p1bJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average set size\n",
        "\n",
        "# Coverage: is y_val[i] contained in confidencesets_classification[i]?\n",
        "\n",
        "# Conditional coverage"
      ],
      "metadata": {
        "id": "51yyZyzQ1bJH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare your results to the Logistic Regression model. What do you notice?"
      ],
      "metadata": {
        "id": "pCkwtrS61iAW"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DSqQzSVrQH0v",
        "YZ4XOM7eunHz",
        "L9ntVH3z7byD"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}